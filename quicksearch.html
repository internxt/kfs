<html>
<head>
</head>
<body style="background: transparent;">
    <script src="scripts/docstrap.lib.js"></script>
    <script src="scripts/lunr.min.js"></script>
    <script src="scripts/fulltext-search.js"></script>

    <script type="text/x-docstrap-searchdb">
    {"lib_block-stream.js.html":{"id":"lib_block-stream.js.html","title":"Source: lib/block-stream.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/block-stream.js 'use strict'; const {Transform: TransformStream} = require('readable-stream'); const merge = require('merge'); /** * Transforms the input stream into an output stream of N-sized chunks */ class BlockStream extends TransformStream { static get DEFAULTS() { return { padLastChunk: false }; } /** * @constructor * @param {Object} [options] * @param {Sbucket} [options.sBucket] - The S-bucket for chunks allocation * @param {Boolean} [options.padLastChunk=false] - Pad last chunk with zeros */ constructor(options) { super(); options = merge(BlockStream.DEFAULTS, options); this._addPadding = options.padLastChunk; this._bufferLength = 0; this._offset = 0; this._inputQueue = []; this._sBucket = options.sBucket; } /** * Triggered when data is available * @event BlockStream#data * @param {Buffer} chunk */ /** * Triggered when the stream is ended * @event BlockStream#end */ /** * Implements the transform method * @private */ _transform(bytes, encoding, callback) { this._addToBuffer(bytes); this._drainInternalBuffer(); callback(null); } /** * Implements the flush method * @private */ _flush(callback) { if(this._bufferLength === 0) { return callback(null); } const chunk = (this._addPadding &amp;&amp; this._sBucket._chunkSize !== this._bufferLength) ? ((this._sBucket._chunkFree.length &gt; 0) ? this._sBucket._chunkFree.shift() : Buffer.allocUnsafe(this._sBucket._chunkSize)) .fill(0, this._bufferLength) : Buffer.allocUnsafe(this._bufferLength); var i = 0; while(this._bufferLength &gt; 0) { const input = this._inputQueue.shift(); const k = (input.length - this._offset); input.copy(chunk, i, this._offset); this._offset = 0; i += k; this._bufferLength -= k; } this.push(chunk); this._sBucket._chunkFree.splice(0, this._sBucket._chunkFree.length); callback(null); } /** * Drains the internal buffer * @private */ _drainInternalBuffer() { const self = this; function _transformChunk(chunk, j) { var i = 0; while (i &lt; self._sBucket._chunkSize) { const input = self._inputQueue.shift(); const k = (input.length - self._offset); if (j &gt;= k) { input.copy(chunk, i, self._offset); self._offset = 0; i += k; j -= k; } else { input.copy(chunk, i, self._offset, self._offset + j); self._inputQueue.unshift(input); self._offset += j; i += j; } } } while (this._bufferLength &gt;= this._sBucket._chunkSize) { const chunk = (this._sBucket._chunkFree.length &gt; 0) ? this._sBucket._chunkFree.shift() : Buffer.allocUnsafe(this._sBucket._chunkSize); _transformChunk(chunk, this._sBucket._chunkSize); this.push(chunk); this._bufferLength -= this._sBucket._chunkSize; } } /** * Adds the bytes to the internal buffer * @private */ _addToBuffer(bytes) { this._inputQueue.push(bytes); this._bufferLength += bytes.length; } } module.exports = BlockStream; Ã— Search results Close "},"lib_b-table.js.html":{"id":"lib_b-table.js.html","title":"Source: lib/b-table.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/b-table.js 'use strict'; const merge = require('merge'); const {EventEmitter} = require('events'); const fs = require('fs'); const mkdirp = require('mkdirp'); const utils = require('./utils'); const constants = require('./constants'); const Sbucket = require('./s-bucket'); const path = require('path'); const assert = require('assert'); const async = require('async'); /** * Represents the primary interface for the KFS file store */ class Btable extends EventEmitter { static get RID_FILENAME() { return 'r.id'; } static get DEFAULTS() { return { referenceId: null, maxTableSize: constants.S * constants.B, sBucketOpts: {} }; } /** * Constructs series of {@link Sbucket}s composing a sharded table * @constructor * @param {String} tablePath - The path to the directory to store the table * @param {Object} [options] * @param {String} [options.referenceId] - R bit hex reference ID * @param {Number} [options.maxTableSize] - Max bytes to cap the database * @param {Object} [options.sBucketOpts] - Options to pass to Sbucket */ constructor(tablePath, options) { super(); this._options = merge(Btable.DEFAULTS, options); this._rid = utils.createReferenceId(this._options.referenceId); this._sBuckets = {}; this._tablePath = utils.coerceTablePath(tablePath); this._maxTableSize = this._options.maxTableSize; this._options.sBucketOpts.maxSize = this._maxTableSize / constants.B; this._open(); } /** * Opens the Btable, creating it if it does not exist * @private */ _open() { if (!utils.fileDoesExist(this._tablePath)) { this._initBtableDirectory(); } else { this._validateTablePath(); } this._rid = Buffer.from(fs.readFileSync( path.join(this._tablePath, Btable.RID_FILENAME), { encoding: 'hex' } ), 'hex'); } /** * Initializes a new KFS database (B-table directory) * @private */ _initBtableDirectory() { mkdirp.sync(this._tablePath); fs.writeFileSync( path.join(this._tablePath, Btable.RID_FILENAME), this._rid, { encoding: 'hex' } ); } /** * Validates a path to a directory as a KFS instance * @private */ _validateTablePath() { const dirStats = fs.statSync(this._tablePath); assert(dirStats.isDirectory(), 'Table path is not a directory'); const requiredPaths = [Btable.RID_FILENAME]; const dirContents = fs.readdirSync(this._tablePath); for (let pathName of requiredPaths) { assert(dirContents.indexOf(pathName) !== -1, 'Table path is not a valid KFS instance'); } } /** * Determine the {@link Sbucket} index for a given key * @private * @param {String} key - The data key to route * @returns {Number} */ _getSbucketIndexForKey(key) { return this._rid[0] ^ Buffer.from(utils.hashKey(key), 'hex')[0]; } /** * Get the {@link Sbucket} for the supplied index * @private * @param {Number} sBucketIndex - The index for the desired bucket * @returns {Sbucket} */ _getSbucketAtIndex(sBucketIndex) { assert(sBucketIndex &lt; constants.B, 'Index must not be greater than B'); assert(sBucketIndex &gt; -1, 'Index must be greater than or equal to 0'); if (this._sBuckets[sBucketIndex]) { return this._sBuckets[sBucketIndex]; } this._sBuckets[sBucketIndex] = new Sbucket( path.join(this._tablePath, utils.createSbucketNameFromIndex(sBucketIndex)), this._options.sBucketOpts ); this._sBuckets[sBucketIndex].removeAllListeners('idle'); this._sBuckets[sBucketIndex].once('idle', () =&gt; { this._sBuckets[sBucketIndex].close(); }); return this._sBuckets[sBucketIndex]; } /** * Get the {@link Sbucket} for the given key * @private * @param {String} key - The key that maps to a {@link Sbucket} * @param {Btable~_getSbucketForKeyCallback} */ _getSbucketForKey(key, callback) { const sIndex = typeof key === 'number' ? key : this._getSbucketIndexForKey(key); const sBucket = this._getSbucketAtIndex(sIndex); if (sBucket.readyState !== Sbucket.OPENED) { return sBucket.open((err) =&gt; { if (err) { return callback(err); } callback(null, sBucket, sIndex); }); } callback(null, sBucket, sIndex); } /** * @private * @callback Btable~_getSbucketForKeyCallback * @param {Error} [error] * @param {Sbucket} sBucket */ /** * Lists the created {@link Sbucket}s and their sizes * @param {String|Number} [keyOrIndex] - Optional bucket index or file key * @param {Btable~statCallback} */ stat(keyOrIndex, callback) { const self = this; if (typeof keyOrIndex === 'function') { callback = keyOrIndex; keyOrIndex = null; } if (keyOrIndex) { return _getStat(keyOrIndex, (err, stats) =&gt; { callback(err, stats ? [stats] : undefined); }); } let sBuckets = fs.readdirSync(this._tablePath) .filter((name) =&gt; name !== Btable.RID_FILENAME) .map((sBucketName) =&gt; parseInt(sBucketName)) .filter((sBucketIndex) =&gt; { return !Number.isNaN(sBucketIndex) &amp;&amp; typeof sBucketIndex === 'number'; }); function _getStat(sBucketIndex, done) { self._getSbucketForKey(sBucketIndex, (err, sBucket, sIndex) =&gt; { if (err) { return done(err); } sBucket.stat((err, stats) =&gt; { if (err) { return done(err); } done(null, { sBucketIndex: sIndex, sBucketStats: stats }); }); }); } async.mapLimit(sBuckets, 3, _getStat, callback); } /** * @callback Btable~statCallback * @param {Error} [error] * @param {Object[]} sBuckets * @param {String} sBuckets[].sBucketIndex - The index of the S-bucket * @param {Object} sBuckets[].sBucketStats * @param {Number} sBuckets[].sBucketStats.used - Space used in the bucket * @param {Number} sBuckets[].sBucketStats.free - Space free in the bucket */ /** * Lists the file keys in the given bucket * @param {Number|String} keyOrIndex - The bucket index of a file key * @param {Sbucket~listCallback} */ list(keyOrIndex, callback) { var key = typeof keyOrIndex === 'number' ? keyOrIndex : utils.coerceKey(keyOrIndex); this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } sBucket.list(callback); }); } /** * Check if a file exists at the supplied key * @param {String} key - The key to check for existence * @param {Sbucket~existsCallback} */ exists(key, callback) { this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } sBucket.exists(key, callback); }); } /** * Unlinks the data for the given key * @param {String} key - The key to unlink data from * @param {Sbucket~unlinkCallback} */ unlink(key, callback) { this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } sBucket.unlink(key, callback); }); } /** * Performs a flush on each S-bucket in the table to free any dead space * @param {Btable~flushCallback} */ flush(callback) { async.eachSeries(Object.keys(this._sBuckets), (k, next) =&gt; { this._getSbucketForKey(parseInt(k), (err, sBucket) =&gt; { sBucket.flush(next); }); }, callback); } /** * @callback Btable~flushCallback * @param {Error|null} error */ /** * Reads the data at the supplied key into a buffer * @param {String} key - The key for the data to read * @param {Sbucket~readFileCallback} */ readFile(key, callback) { this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } sBucket.readFile(key, callback); }); } /** * Creates a readable stream of the data at the given key * @param {String} key - The key for the data read * @param {Btable~createReadStreamCallback} */ createReadStream(key, callback) { this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } callback(null, sBucket.createReadStream(key)); }); } /** * @callback Btable~createReadStreamCallback * @param {Error} [error] * @param {ReadableStream} readStream */ /** * Writes the given buffer to the key * @param {String} key - The key to write the data to * @param {Buffer} buffer - The raw buffer to write to the key * @param {Sbucket~writeFileCallback} */ writeFile(key, buffer, callback) { this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } sBucket.writeFile(key, buffer, callback); }); } /** * Creates a writable stream to the given key * @param {String} key - The key to write the data to * @param {Btable~createWriteStreamCallback} */ createWriteStream(key, callback) { this._getSbucketForKey(key, (err, sBucket) =&gt; { if (err) { return callback(err); } callback(null, sBucket.createWriteStream(key)); }); } /** * @callback Btable~createWriteStreamCallback * @param {Error} [error] * @param {WritableStream} writeStream */ } module.exports = Btable; Ã— Search results Close "},"index.js.html":{"id":"index.js.html","title":"Source: index.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: index.js /** * @module kfs */ 'use strict'; const Btable = require('./lib/b-table'); /** * Returns a constructed {@link Btable} * @function * @param {string} path - Path to the KFS store * @param {object} [options] - {@link Btable} options */ module.exports = (path, opts) =&gt; new Btable(path, opts); /** {@link Btable} */ module.exports.Btable = Btable; /** {@link Sbucket} */ module.exports.Sbucket = require('./lib/s-bucket'); /** {@link BlockStream} */ module.exports.BlockStream = require('./lib/block-stream'); /** {@link ReadableFileStream} */ module.exports.ReadableFileStream = require('./lib/read-stream'); /** {@link WritableFileStream} */ module.exports.WritableFileStream = require('./lib/write-stream'); /** {@link module:kfs/constants} */ module.exports.constants = require('./lib/constants'); /** {@link module:kfs/utils} */ module.exports.utils = require('./lib/utils'); Ã— Search results Close "},"lib_constants.js.html":{"id":"lib_constants.js.html","title":"Source: lib/constants.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/constants.js /** * @module kfs/constants */ 'use strict'; module.exports = Object.freeze({ /** @constant {Number} R - Number of bits in Reference ID */ R: 160, /** @constant {Number} C - Number of bytes in a file chunk */ C: 131072, /** @constant {Number} S - Number of bytes in a {@link Sbucket} */ S: 32 * (1024 * 1024 * 1024), /** @constant {Number} B - Number of columns in a {@link Btable} */ B: 256, /** @constant {String} HASH - OpenSSL id for key hashing algorithm */ HASH: 'rmd160', /** @constant {Number} SBUCKET_IDLE - Time to wait before idle event */ SBUCKET_IDLE: 60000 }); Ã— Search results Close "},"lib_utils.js.html":{"id":"lib_utils.js.html","title":"Source: lib/utils.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/utils.js /** * @module kfs/utils */ 'use strict'; const assert = require('assert'); const constants = require('./constants'); const fs = require('fs'); const crypto = require('crypto'); const path = require('path'); /** * A stubbed noop function */ module.exports.noop = function() {}; /** * Tests if the string is a valid key * @param {String} key - The file key * @returns {Boolean} */ module.exports.isValidKey = function(key) { let keyBuffer; try { keyBuffer = Buffer.from(key, 'hex'); } catch (err) { return false; } return keyBuffer.length === (constants.R / 8); }; /** * Hashes the given key * @param {String} key - The file key * @returns {String} */ module.exports.hashKey = function(key) { if (module.exports.isValidKey(key)) { return key; } return crypto.createHash(constants.HASH).update(key).digest('hex'); }; /** * Coerces input into a valid file key * @param {String} key - The file key * @returns {String} */ module.exports.coerceKey = function(key) { if (!module.exports.isValidKey(key)) { return module.exports.hashKey(key); } return key; }; /** * Get the key name for a data hash + index * @param {String} key - Hash of the data * @param {Number} index - The index of the file chunk * @returns {String} */ module.exports.createItemKeyFromIndex = function(key, index) { assert(typeof index === 'number', 'Invalid index supplied'); const fileKey = module.exports.hashKey(key); const indexLength = Math.floor(constants.S / constants.C).toString().length; const indexString = index.toString(); let itemIndex = ''; assert(Buffer.from(fileKey, 'hex').length * 8 === constants.R, 'Invalid key'); assert(indexString.length &lt;= indexLength, 'Index is out of bounds'); for (var i = 0; i &lt; indexLength - indexString.length; i++) { itemIndex += '0'; } itemIndex += indexString; return `${fileKey} ${itemIndex}`; }; /** * Get the file name of an s bucket based on it's index * @param {Number} sBucketIndex - The index fo the bucket in the B-table * @returns {String} */ module.exports.createSbucketNameFromIndex = function(sBucketIndex) { assert(typeof sBucketIndex === 'number', 'Invalid index supplied'); const indexLength = constants.B.toString().length; const indexString = sBucketIndex.toString(); let leadingZeroes = ''; for (var i = 0; i &lt; indexLength - indexString.length; i++) { leadingZeroes += '0'; } return `${leadingZeroes}${indexString}.s`; }; /** * Creates a random reference ID * @param {String} [rid] - An existing hex reference ID * @returns {String} */ module.exports.createReferenceId = function(rid) { if (!rid) { rid = crypto.randomBytes(constants.R / 8).toString('hex'); } assert(rid.length === 40, 'Invalid reference ID length'); return Buffer.from(rid, 'hex'); }; /** * Check if the given path exists * @param {String} filePath * @returns {Boolean} */ module.exports.fileDoesExist = function(filePath) { try { fs.statSync(filePath); } catch (err) { return false; } return true; }; /** * Takes a number of bytes and outputs a human readable size * @param {Number} bytes - The number of bytes to make readable * @returns {String} */ module.exports.toHumanReadableSize = function(bytes) { const thresh = 1024; if (Math.abs(bytes) &lt; thresh) { return bytes + ' B'; } const units = ['KiB','MiB','GiB','TiB','PiB','EiB','ZiB','YiB']; let u = -1; do { bytes /= thresh; ++u; } while (Math.abs(bytes) &gt;= thresh &amp;&amp; u &lt; units.length - 1); return `${bytes.toFixed(1)} ${units[u]}`; }; /** * Ensures that the given path has a kfs extension * @param {String} tablePath - The path name to a kfs instance * @returns {String} */ module.exports.coerceTablePath = function(tablePath) { if (path.extname(tablePath) !== '.kfs') { return `${tablePath}.kfs`; } return tablePath; }; /** * Determines if the passed error object is a NotFound error * @param {Error} error * @returns {Boolean} */ module.exports.isNotFoundError = function(error) { return error &amp;&amp; error.message.indexOf('NotFound:') !== -1; }; Ã— Search results Close "},"lib_read-stream.js.html":{"id":"lib_read-stream.js.html","title":"Source: lib/read-stream.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/read-stream.js 'use strict'; const {Readable: ReadableStream} = require('readable-stream').Readable; const utils = require('./utils'); /** * Creates a readable stream of a file from a {@link Sbucket} */ class ReadableFileStream extends ReadableStream { /** * @constructor * @param {Object} options * @param {Sbucket} options.sBucket * @param {String} options.fileKey */ constructor(options) { super(); this._sBucket = options.sBucket; this._fileKey = options.fileKey; this._index = 0; } /** * Triggered when data is available to read * @event ReadableFileStream#readable */ /** * Triggered when a data is pushed through the stream * @event ReadableFileStream#data * @param {Buffer} bytes */ /** * Triggered when no more data is available * @event ReadableFileStream#end */ /** * Triggered if an error occurs * @event ReadableFileStream#error * @param {Error} error */ /** * @private */ _read() { const itemKey = utils.createItemKeyFromIndex(this._fileKey, this._index); this._sBucket._db.get(itemKey, (err, result) =&gt; { if (err) { if (utils.isNotFoundError(err)) { return this.push(null); } else { return this.emit('error', err); } } this._index++; this.push(Buffer.from(result, 'binary')); }); } /** * Destroys and aborts any reads for this stream * @param {Sbucket~unlinkCallback} */ destroy(callback) { this._sBucket.unlink(this._fileKey, callback); } } module.exports = ReadableFileStream; Ã— Search results Close "},"lib_s-bucket.js.html":{"id":"lib_s-bucket.js.html","title":"Source: lib/s-bucket.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/s-bucket.js 'use strict'; const merge = require('merge'); const leveldown = require('leveldown'); const {EventEmitter} = require('events'); const constants = require('./constants'); const utils = require('./utils'); const WritableFileStream = require('./write-stream'); const ReadableFileStream = require('./read-stream'); const BlockStream = require('./block-stream'); const async = require('async'); /** * Capped LevelDB database within a {@link Btable} */ class Sbucket extends EventEmitter { static get CLOSED() { return 4; } static get CLOSING() { return 3; } static get OPENED() { return 2; } static get OPENING() { return 1; } static get SIZE_START_KEY() { return '0'; } static get SIZE_END_KEY() { return 'z'; } static get DEFAULTS() { return { maxOpenFiles: 1000, compression: false, cacheSize: 8 * (1024 * 1024), createIfMissing: true, errorIfExists: false, writeBufferSize: 4 * (1024 * 1024), blockSize: 4096, blockRestartInterval: 16, maxSize: constants.S, chunkSize: constants.C }; } /** * @constructor * @param {String} dbPath - The path to database on disk * @param {Object} [options] - Options to pass through to leveldown#open * @param {Number} [options.maxOpenFiles=1000] * @param {Boolean} [options.compression=false] * @param {Number} [options.cacheSize=8388608] * @param {Boolean} [options.createIfMissing=true] * @param {Boolean} [options.errorIfExists=false] * @param {Number} [options.writeBufferSize=4194304] * @param {Number} [options.blockSize=4096] * @param {Number} [options.blockRestartInterval=16] */ constructor(dbPath, options) { super(); this.setMaxListeners(Infinity); this._dbPath = dbPath; this._options = merge(Sbucket.DEFAULTS, options); this._db = leveldown(dbPath); this._pendingOperations = 0; this._maxSize = this._options.maxSize; this.readyState = Sbucket.CLOSED; this._chunkFree = []; this._chunkSize = this._options.chunkSize; } /** * Triggered when the underlying database opens * @event Sbucket#open */ /** * Triggered when the underlying database closes * @event Sbucket#close */ /** * Triggered when there are no more pending operations * @event Sbucket#idle */ /** * Triggered when the bucket is locked for flushing * @event Sbucket#locked */ /** * Triggered when the bucket is unlocked * @event Sbucket#unlocked */ /** * Opens the underlying database * @fires Sbucket#open * @param {Sbucket~openCallback} */ open(callback=utils.noop) { const self = this; function _open() { self.readyState = Sbucket.OPENING; self._db.open(self._options, function(err) { if (err) { return self.emit('error', err); } self.readyState = Sbucket.OPENED; self.emit('open'); self._idleCheckInterval = setInterval( () =&gt; self._checkIdleState(), constants.SBUCKET_IDLE ); }); } function _onError(err) { self.removeListener('open', _onOpen); callback(err); } function _onOpen() { self.removeListener('error', _onError); callback(null); } this.once('open', _onOpen).once('error', _onError); if (this.readyState === Sbucket.OPENED) { return this.emit('open'); } if (this.readyState === Sbucket.OPENING) { return; } if (this.readyState === Sbucket.CLOSING) { return this.once('close', _open); } _open(); } /** * @callback Sbucket~openCallback * @param {Error} [error] */ /** * Closes the underlying database * @fires Sbucket#close * @param {Sbucket~closeCallback} */ close(callback=utils.noop) { const self = this; function _close() { self.readyState = Sbucket.CLOSING; self._db.close(function(err) { if (err) { return self.emit('error', err); } self.readyState = Sbucket.CLOSED; self.emit('close'); clearInterval(self._idleCheckInterval); }); } function _onError(err) { self.removeListener('close', _onClose); callback(err); } function _onClose() { self.removeListener('error', _onError); callback(null); } this.once('close', _onClose).once('error', _onError); if (this.readyState === Sbucket.CLOSED) { return this.emit('close'); } if (this.readyState === Sbucket.CLOSING) { return; } if (this.readyState === Sbucket.OPENING) { return this.once('open', _close); } _close(); } /** * @callback Sbucket~closeCallback * @param {Error} [error] */ /** * Determines if the file is already stored in the db * @param {String} key - The key for the file stored * @param {Sbucket~existsCallback} */ exists(key, callback) { this._incPendingOps(); this._db.get(utils.createItemKeyFromIndex(key, 0), (err) =&gt; { this._decPendingOps(); callback(null, !err); }); } /** * @callback Sbucket~existsCallback * @param {Error} [error] * @param {Boolean} fileDoesExist */ /** * Deletes the file chunks from the database * @param {String} key - The key for the file stored * @param {Sbucket~unlinkCallback} */ unlink(key, callback) { const self = this; let index = 0; function _del(index, callback) { const itemKey = utils.createItemKeyFromIndex(key, index); self._db.get(itemKey, function(err) { index++; if (!err) { self._db.del(itemKey, () =&gt; _del(index, callback)); } else if (utils.isNotFoundError(err)) { self._decPendingOps(); callback(null); } else { self._decPendingOps(); callback(err); } }); } this._incPendingOps(); _del(index, callback); } /** * @callback Sbucket~unlinkCallback * @param {Error} [error] */ /** * Reads the file at the given key into a buffer * @param {String} key - The key for the file to read * @param {Sbucket~readFileCallback} */ readFile(key, callback) { let fileBuffer = Buffer.from([], 'binary'); const readStream = this.createReadStream(key); readStream.on('data', (data) =&gt; { fileBuffer = Buffer.concat([fileBuffer, data]); }); readStream.on('end', () =&gt; { this._decPendingOps(); callback(null, fileBuffer); }); readStream.on('error', (err) =&gt; { this._decPendingOps(); readStream.removeAllListeners(); callback(err); }); this._incPendingOps(); } /** * @callback Sbucket~readFileCallback * @param {Error} [error] * @param {Buffer} fileBuffer */ /** * Writes the buffer to the given key * @param {String} key - The key for the file to write * @param {Buffer} buffer - The data to write to the given key * @param {Sbucket~writeFileCallback} */ writeFile(key, buffer, callback) { const self = this; const writeStream = this.createWriteStream(key); let whichSlice = 0; function _writeFileSlice() { var startIndex = whichSlice * self._options.chunkSize; var endIndex = startIndex + self._options.chunkSize; var bufferSlice = buffer.slice(startIndex, endIndex); if (bufferSlice.length === 0) { return writeStream.end(); } whichSlice++; writeStream.write(bufferSlice); _writeFileSlice(); } writeStream.on('finish', () =&gt; { this._decPendingOps(); callback(null); }); writeStream.on('error', (err) =&gt; { this._decPendingOps(); writeStream.removeAllListeners(); callback(err); }); this._incPendingOps(); this.unlink(key, _writeFileSlice); } /** * @callback Sbucket~writeFileCallback * @param {Error} [error] */ /** * Returns a readable stream of the file at the given key * @param {String} key - The key for the file to read * @returns {ReadableFileStream} */ createReadStream(key) { const rs = new ReadableFileStream({ sBucket: this, fileKey: key }); this._incPendingOps(); rs.on('end', () =&gt; this._decPendingOps()); return rs; } /** * Returns a writable stream for a file at the given key * @param {String} key - The key for the file to read * @returns {WritableFileStream} */ createWriteStream(key) { const bs = new BlockStream({ padLastChunk: false, sBucket: this }); const ws = new WritableFileStream({ sBucket: this, fileKey: key }); // NB: Expose the underyling writable stream's #destroy method bs.destroy = (cb) =&gt; ws.destroy(cb); this._incPendingOps(); bs.pipe(ws).on('finish', () =&gt; this._decPendingOps()); return bs; } /** * Get stats for this bucket * @param {Sbucket~statCallback} */ stat(callback) { const [start, end] = [Sbucket.SIZE_START_KEY, Sbucket.SIZE_END_KEY]; this._incPendingOps(); this._db.approximateSize(start, end, (err, size) =&gt; { this._decPendingOps(); if (err) { return callback(err); } callback(null, { size: size, free: this._maxSize - size }); }); } /** * @callback Sbucket~statCallback * @param {Error} [error] * @param {Object} bucketStats * @param {Number} bucketStats.size - The used space in bytes * @param {Number} bucketStats.free - The free space left in bytes */ /** * Get a list of file keys in the bucket and their approximate size * @param {Sbucket~listCallback} */ list(callback) { const self = this; const iterator = this._db.iterator({ gte: Sbucket.SIZE_START_KEY, lte: Sbucket.SIZE_END_KEY, values: false, keyAsBuffer: false }); const keys = {}; let currentResult = null; function _test() { return currentResult === null; } function _accumulateKey(next) { iterator.next((err, key) =&gt; { if (err) { return next(err); } if (!key) { currentResult = null; return next(); } currentResult = key.split(' ')[0]; keys[currentResult] = keys[currentResult] ? keys[currentResult] + self._options.chunkSize : self._options.chunkSize; next(); }); } this._incPendingOps(); async.doUntil(_accumulateKey, _test, (err) =&gt; { this._decPendingOps(); if (err) { return callback(err); } var results = []; for (var key in keys) { results.push({ baseKey: key, approximateSize: keys[key] }); } callback(null, results); }); } /** * @callback Sbucket~listCallback * @param {Error} [error] * @param {Object[]} results * @param {String} results.baseKey * @param {Number} results.approximateSize */ /** * Trigger a compaction for the S-bucket * @param {Sbucket~flushCallback} */ flush(callback) { this._db.compactRange(Sbucket.SIZE_START_KEY, Sbucket.SIZE_END_KEY, callback); } /** * @callback Sbucket~flushCallback * @param {Error|null} error */ /** * Increments the pending operations counter * @private */ _incPendingOps() { this._pendingOperations++; } /** * Decrements the pending operations counter * @private * @fires Sbucket#idle */ _decPendingOps() { this._pendingOperations--; setImmediate(() =&gt; this._checkIdleState()); } /** * Emits the idle event if state is idle * @private */ _emitIfStateIsIdle() { if (this._pendingOperations === 0) { this.emit('idle'); return true; } return false; } /** * Checks the idle state and triggers a timeout for emitting the idle event * @private * @returns {Boolean} hasNoPendingOperations */ _checkIdleState() { if (this._pendingOperations !== 0) { return false; } setTimeout(() =&gt; this._emitIfStateIsIdle(), constants.SBUCKET_IDLE); return true; } } module.exports = Sbucket; Ã— Search results Close "},"lib_write-stream.js.html":{"id":"lib_write-stream.js.html","title":"Source: lib/write-stream.js","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Source: lib/write-stream.js 'use strict'; const {Writable: WritableStream} = require('readable-stream'); const utils = require('./utils'); /** * Creates a writable stream for storing a file in an {@link Sbucket} */ class WritableFileStream extends WritableStream { /** * @constructor * @param {Object} options * @param {Sbucket} options.sBucket - The S-bucket this stream will write to * @param {String} options.fileKey - The key for the file to write to */ constructor(options) { super(); this._sBucket = options.sBucket; this._fileKey = options.fileKey; this._index = 0; } /** * Triggered if an error occurs * @event WritableFileStream#error * @param {Error} error */ /** * Triggered when data is finished writing * @event WritableFileStream#finish */ /** * @private */ _write(bytes, encoding, callback) { const itemKey = utils.createItemKeyFromIndex(this._fileKey, this._index); this._sBucket._db.put(itemKey, bytes, (err) =&gt; { if(bytes.length === this._sBucket._chunkSize) { this._sBucket._chunkFree.push(bytes); } if (err) { return callback(err); } this._index++; callback(); }); } /** * Destroys and aborts any writes for this stream * @param {Sbucket~unlinkCallback} */ destroy(callback) { this._sBucket.unlink(this._fileKey, callback); } } module.exports = WritableFileStream; Ã— Search results Close "},"modules.list.html":{"id":"modules.list.html","title":"Modules","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Modules Classes BlockStream Btable ReadableFileStream Sbucket WritableFileStream Events data Triggered when data is available Parameters: Name Type Description chunk Buffer Source: lib/block-stream.js end Triggered when the stream is ended Source: lib/block-stream.js data Triggered when a data is pushed through the stream Parameters: Name Type Description bytes Buffer Source: lib/read-stream.js end Triggered when no more data is available Source: lib/read-stream.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/read-stream.js readable Triggered when data is available to read Source: lib/read-stream.js close Triggered when the underlying database closes Source: lib/s-bucket.js idle Triggered when there are no more pending operations Source: lib/s-bucket.js locked Triggered when the bucket is locked for flushing Source: lib/s-bucket.js open Triggered when the underlying database opens Source: lib/s-bucket.js unlocked Triggered when the bucket is unlocked Source: lib/s-bucket.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/write-stream.js finish Triggered when data is finished writing Source: lib/write-stream.js Ã— Search results Close "},"classes.list.html":{"id":"classes.list.html","title":"Classes","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Classes Classes BlockStream Btable ReadableFileStream Sbucket WritableFileStream Events data Triggered when data is available Parameters: Name Type Description chunk Buffer Source: lib/block-stream.js end Triggered when the stream is ended Source: lib/block-stream.js data Triggered when a data is pushed through the stream Parameters: Name Type Description bytes Buffer Source: lib/read-stream.js end Triggered when no more data is available Source: lib/read-stream.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/read-stream.js readable Triggered when data is available to read Source: lib/read-stream.js close Triggered when the underlying database closes Source: lib/s-bucket.js idle Triggered when there are no more pending operations Source: lib/s-bucket.js locked Triggered when the bucket is locked for flushing Source: lib/s-bucket.js open Triggered when the underlying database opens Source: lib/s-bucket.js unlocked Triggered when the bucket is unlocked Source: lib/s-bucket.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/write-stream.js finish Triggered when data is finished writing Source: lib/write-stream.js Ã— Search results Close "},"tutorials.list.html":{"id":"tutorials.list.html","title":"Tutorials","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Tutorials Classes BlockStream Btable ReadableFileStream Sbucket WritableFileStream Events data Triggered when data is available Parameters: Name Type Description chunk Buffer Source: lib/block-stream.js end Triggered when the stream is ended Source: lib/block-stream.js data Triggered when a data is pushed through the stream Parameters: Name Type Description bytes Buffer Source: lib/read-stream.js end Triggered when no more data is available Source: lib/read-stream.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/read-stream.js readable Triggered when data is available to read Source: lib/read-stream.js close Triggered when the underlying database closes Source: lib/s-bucket.js idle Triggered when there are no more pending operations Source: lib/s-bucket.js locked Triggered when the bucket is locked for flushing Source: lib/s-bucket.js open Triggered when the underlying database opens Source: lib/s-bucket.js unlocked Triggered when the bucket is unlocked Source: lib/s-bucket.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/write-stream.js finish Triggered when data is finished writing Source: lib/write-stream.js Ã— Search results Close "},"index.html":{"id":"index.html","title":"Index","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes KFS (Kademlia File Store) The KFS system describes a method for managing the storage layer of nodes on the Storj Network by creating a sharded local database where content-addressable data is placed in a shard using the same routing metric and algorithm used by the Kademlia distributed hash table. Be sure to read about the motivation and how it works! Quick Start Install the kfs package using Node Package Manager. npm install kfs --save This will install kfs as a dependency of your own project. See the documentation for in-depth usage details. You can also install globally to use the kfs command line utility. const kfs = require('kfs'); const store = kfs('path/to/store'); store.writeFile('some key', Buffer.from('some data'), (err) =&gt; { console.log(err || 'File written to store!'); }); License KFS - A Local File Storage System Inspired by Kademlia Copyright (C) 2016 Storj Labs, Inc This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see [http://www.gnu.org/licenses/]. Ã— Search results Close "},"BlockStream.html":{"id":"BlockStream.html","title":"Class: BlockStream","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Class: BlockStream BlockStream Transforms the input stream into an output stream of N-sized chunks new BlockStream( [options]) Parameters: Name Type Argument Description options Object &lt;optional&gt; Properties Name Type Argument Default Description sBucket Sbucket &lt;optional&gt; The S-bucket for chunks allocation padLastChunk Boolean &lt;optional&gt; false Pad last chunk with zeros Source: lib/block-stream.js Events data Triggered when data is available Parameters: Name Type Description chunk Buffer Source: lib/block-stream.js end Triggered when the stream is ended Source: lib/block-stream.js Ã— Search results Close "},"Btable.html":{"id":"Btable.html","title":"Class: Btable","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Class: Btable Btable Represents the primary interface for the KFS file store new Btable(tablePath [, options]) Constructs series of Sbuckets composing a sharded table Parameters: Name Type Argument Description tablePath String The path to the directory to store the table options Object &lt;optional&gt; Properties Name Type Argument Description referenceId String &lt;optional&gt; R bit hex reference ID maxTableSize Number &lt;optional&gt; Max bytes to cap the database sBucketOpts Object &lt;optional&gt; Options to pass to Sbucket Source: lib/b-table.js Methods createReadStream(key, callback) Creates a readable stream of the data at the given key Parameters: Name Type Description key String The key for the data read callback Btable~createReadStreamCallback Source: lib/b-table.js createWriteStream(key, callback) Creates a writable stream to the given key Parameters: Name Type Description key String The key to write the data to callback Btable~createWriteStreamCallback Source: lib/b-table.js exists(key, callback) Check if a file exists at the supplied key Parameters: Name Type Description key String The key to check for existence callback Sbucket~existsCallback Source: lib/b-table.js flush(callback) Performs a flush on each S-bucket in the table to free any dead space Parameters: Name Type Description callback Btable~flushCallback Source: lib/b-table.js list(keyOrIndex, callback) Lists the file keys in the given bucket Parameters: Name Type Description keyOrIndex Number | String The bucket index of a file key callback Sbucket~listCallback Source: lib/b-table.js readFile(key, callback) Reads the data at the supplied key into a buffer Parameters: Name Type Description key String The key for the data to read callback Sbucket~readFileCallback Source: lib/b-table.js stat( [keyOrIndex], callback) Lists the created Sbuckets and their sizes Parameters: Name Type Argument Description keyOrIndex String | Number &lt;optional&gt; Optional bucket index or file key callback Btable~statCallback Source: lib/b-table.js unlink(key, callback) Unlinks the data for the given key Parameters: Name Type Description key String The key to unlink data from callback Sbucket~unlinkCallback Source: lib/b-table.js writeFile(key, buffer, callback) Writes the given buffer to the key Parameters: Name Type Description key String The key to write the data to buffer Buffer The raw buffer to write to the key callback Sbucket~writeFileCallback Source: lib/b-table.js Type Definitions createReadStreamCallback( [error], readStream) Parameters: Name Type Argument Description error Error &lt;optional&gt; readStream ReadableStream Source: lib/b-table.js flushCallback(error) Parameters: Name Type Description error Error | null Source: lib/b-table.js statCallback( [error], sBuckets) Parameters: Name Type Argument Description error Error &lt;optional&gt; sBuckets Array.&lt;Object&gt; sBuckets[].sBucketIndex String The index of the S-bucket sBuckets[].sBucketStats Object Properties Name Type Description used Number Space used in the bucket free Number Space free in the bucket Source: lib/b-table.js Ã— Search results Close "},"module-kfs.html":{"id":"module-kfs.html","title":"Module: kfs","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Module: kfs (require(\"kfs\"))(path [, options]) Returns a constructed Btable Parameters: Name Type Argument Description path string Path to the KFS store options object &lt;optional&gt; Btable options Source: index.js Members &lt;static&gt; BlockStream BlockStream Source: index.js &lt;static&gt; Btable Btable Source: index.js &lt;static&gt; constants module:kfs/constants Source: index.js &lt;static&gt; ReadableFileStream ReadableFileStream Source: index.js &lt;static&gt; Sbucket Sbucket Source: index.js &lt;static&gt; utils module:kfs/utils Source: index.js &lt;static&gt; WritableFileStream WritableFileStream Source: index.js Ã— Search results Close "},"module-kfs_constants.html":{"id":"module-kfs_constants.html","title":"Module: kfs/constants","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Module: kfs/constants Source: lib/constants.js Members &lt;inner, constant&gt; B :Number Number of columns in a Btable Type: Number Source: lib/constants.js &lt;inner, constant&gt; C :Number Number of bytes in a file chunk Type: Number Source: lib/constants.js &lt;inner, constant&gt; HASH :String OpenSSL id for key hashing algorithm Type: String Source: lib/constants.js &lt;inner, constant&gt; R :Number Number of bits in Reference ID Type: Number Source: lib/constants.js &lt;inner, constant&gt; S :Number Number of bytes in a Sbucket Type: Number Source: lib/constants.js &lt;inner, constant&gt; SBUCKET_IDLE :Number Time to wait before idle event Type: Number Source: lib/constants.js Ã— Search results Close "},"module-kfs_utils.html":{"id":"module-kfs_utils.html","title":"Module: kfs/utils","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Module: kfs/utils Source: lib/utils.js Methods &lt;static&gt; coerceKey(key) Coerces input into a valid file key Parameters: Name Type Description key String The file key Source: lib/utils.js Returns: Type String &lt;static&gt; coerceTablePath(tablePath) Ensures that the given path has a kfs extension Parameters: Name Type Description tablePath String The path name to a kfs instance Source: lib/utils.js Returns: Type String &lt;static&gt; createItemKeyFromIndex(key, index) Get the key name for a data hash + index Parameters: Name Type Description key String Hash of the data index Number The index of the file chunk Source: lib/utils.js Returns: Type String &lt;static&gt; createReferenceId( [rid]) Creates a random reference ID Parameters: Name Type Argument Description rid String &lt;optional&gt; An existing hex reference ID Source: lib/utils.js Returns: Type String &lt;static&gt; createSbucketNameFromIndex(sBucketIndex) Get the file name of an s bucket based on it's index Parameters: Name Type Description sBucketIndex Number The index fo the bucket in the B-table Source: lib/utils.js Returns: Type String &lt;static&gt; fileDoesExist(filePath) Check if the given path exists Parameters: Name Type Description filePath String Source: lib/utils.js Returns: Type Boolean &lt;static&gt; hashKey(key) Hashes the given key Parameters: Name Type Description key String The file key Source: lib/utils.js Returns: Type String &lt;static&gt; isNotFoundError(error) Determines if the passed error object is a NotFound error Parameters: Name Type Description error Error Source: lib/utils.js Returns: Type Boolean &lt;static&gt; isValidKey(key) Tests if the string is a valid key Parameters: Name Type Description key String The file key Source: lib/utils.js Returns: Type Boolean &lt;static&gt; noop() A stubbed noop function Source: lib/utils.js &lt;static&gt; toHumanReadableSize(bytes) Takes a number of bytes and outputs a human readable size Parameters: Name Type Description bytes Number The number of bytes to make readable Source: lib/utils.js Returns: Type String Ã— Search results Close "},"ReadableFileStream.html":{"id":"ReadableFileStream.html","title":"Class: ReadableFileStream","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Class: ReadableFileStream ReadableFileStream Creates a readable stream of a file from a Sbucket new ReadableFileStream(options) Parameters: Name Type Description options Object Properties Name Type Description sBucket Sbucket fileKey String Source: lib/read-stream.js Methods destroy(callback) Destroys and aborts any reads for this stream Parameters: Name Type Description callback Sbucket~unlinkCallback Source: lib/read-stream.js Events data Triggered when a data is pushed through the stream Parameters: Name Type Description bytes Buffer Source: lib/read-stream.js end Triggered when no more data is available Source: lib/read-stream.js error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/read-stream.js readable Triggered when data is available to read Source: lib/read-stream.js Ã— Search results Close "},"Sbucket.html":{"id":"Sbucket.html","title":"Class: Sbucket","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Class: Sbucket Sbucket Capped LevelDB database within a Btable new Sbucket(dbPath [, options]) Parameters: Name Type Argument Description dbPath String The path to database on disk options Object &lt;optional&gt; Options to pass through to leveldown#open Properties Name Type Argument Default Description maxOpenFiles Number &lt;optional&gt; 1000 compression Boolean &lt;optional&gt; false cacheSize Number &lt;optional&gt; 8388608 createIfMissing Boolean &lt;optional&gt; true errorIfExists Boolean &lt;optional&gt; false writeBufferSize Number &lt;optional&gt; 4194304 blockSize Number &lt;optional&gt; 4096 blockRestartInterval Number &lt;optional&gt; 16 Source: lib/s-bucket.js Methods close(callback) Closes the underlying database Parameters: Name Type Description callback Sbucket~closeCallback Source: lib/s-bucket.js Fires: Sbucket#event:close createReadStream(key) Returns a readable stream of the file at the given key Parameters: Name Type Description key String The key for the file to read Source: lib/s-bucket.js Returns: Type ReadableFileStream createWriteStream(key) Returns a writable stream for a file at the given key Parameters: Name Type Description key String The key for the file to read Source: lib/s-bucket.js Returns: Type WritableFileStream exists(key, callback) Determines if the file is already stored in the db Parameters: Name Type Description key String The key for the file stored callback Sbucket~existsCallback Source: lib/s-bucket.js flush(callback) Trigger a compaction for the S-bucket Parameters: Name Type Description callback Sbucket~flushCallback Source: lib/s-bucket.js list(callback) Get a list of file keys in the bucket and their approximate size Parameters: Name Type Description callback Sbucket~listCallback Source: lib/s-bucket.js open(callback) Opens the underlying database Parameters: Name Type Description callback Sbucket~openCallback Source: lib/s-bucket.js Fires: Sbucket#event:open readFile(key, callback) Reads the file at the given key into a buffer Parameters: Name Type Description key String The key for the file to read callback Sbucket~readFileCallback Source: lib/s-bucket.js stat(callback) Get stats for this bucket Parameters: Name Type Description callback Sbucket~statCallback Source: lib/s-bucket.js unlink(key, callback) Deletes the file chunks from the database Parameters: Name Type Description key String The key for the file stored callback Sbucket~unlinkCallback Source: lib/s-bucket.js writeFile(key, buffer, callback) Writes the buffer to the given key Parameters: Name Type Description key String The key for the file to write buffer Buffer The data to write to the given key callback Sbucket~writeFileCallback Source: lib/s-bucket.js Type Definitions closeCallback( [error]) Parameters: Name Type Argument Description error Error &lt;optional&gt; Source: lib/s-bucket.js existsCallback( [error], fileDoesExist) Parameters: Name Type Argument Description error Error &lt;optional&gt; fileDoesExist Boolean Source: lib/s-bucket.js flushCallback(error) Parameters: Name Type Description error Error | null Source: lib/s-bucket.js listCallback( [error], results) Parameters: Name Type Argument Description error Error &lt;optional&gt; results Array.&lt;Object&gt; Properties Name Type Description baseKey String approximateSize Number Source: lib/s-bucket.js openCallback( [error]) Parameters: Name Type Argument Description error Error &lt;optional&gt; Source: lib/s-bucket.js readFileCallback( [error], fileBuffer) Parameters: Name Type Argument Description error Error &lt;optional&gt; fileBuffer Buffer Source: lib/s-bucket.js statCallback( [error], bucketStats) Parameters: Name Type Argument Description error Error &lt;optional&gt; bucketStats Object Properties Name Type Description size Number The used space in bytes free Number The free space left in bytes Source: lib/s-bucket.js unlinkCallback( [error]) Parameters: Name Type Argument Description error Error &lt;optional&gt; Source: lib/s-bucket.js writeFileCallback( [error]) Parameters: Name Type Argument Description error Error &lt;optional&gt; Source: lib/s-bucket.js Events close Triggered when the underlying database closes Source: lib/s-bucket.js idle Triggered when there are no more pending operations Source: lib/s-bucket.js locked Triggered when the bucket is locked for flushing Source: lib/s-bucket.js open Triggered when the underlying database opens Source: lib/s-bucket.js unlocked Triggered when the bucket is unlocked Source: lib/s-bucket.js Ã— Search results Close "},"WritableFileStream.html":{"id":"WritableFileStream.html","title":"Class: WritableFileStream","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Class: WritableFileStream WritableFileStream Creates a writable stream for storing a file in an Sbucket new WritableFileStream(options) Parameters: Name Type Description options Object Properties Name Type Description sBucket Sbucket The S-bucket this stream will write to fileKey String The key for the file to write to Source: lib/write-stream.js Methods destroy(callback) Destroys and aborts any writes for this stream Parameters: Name Type Description callback Sbucket~unlinkCallback Source: lib/write-stream.js Events error Triggered if an error occurs Parameters: Name Type Description error Error Source: lib/write-stream.js finish Triggered when data is finished writing Source: lib/write-stream.js Ã— Search results Close "},"tutorial-about.html":{"id":"tutorial-about.html","title":"Tutorial: Motivation and Mechanics","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Motivation and Mechanics The Storj network consists of a number of distributed peers who provide storage capacity for lease to others. In its current implementation, these nodes store encrypted shards and their associated metadata in a [LevelDB]. LevelDB provides a number of features that make it desirable for this use case; this includes its lexicographically sorted keys providing fast lookups for content-addressable values, fast and efficient compression, and perhaps most notably its portability which allows the Storj software to run on a wide range of hardware including dated or underpowered computers. However, due to the nature of LevelDB's design and its implementation in the Storj software, its performance suffers after the size of the database exceeds approximately 100GiB. This impact is larger on lower end systems and can also vary based on the type of disk in use. These performance issues seem to arise from LevelDB's compaction mechanism (which is an otherwise desirable feature). In addition to the cost of compaction, LevelDB blocks reads and writes during this process, which causes storage nodes to become effectively offline until the process completes. These properties indicate that if the size of a single database can be given an upper bound, then the cost of compaction can be significantly reduced to an acceptable level. Futhermore, in using a single database, if one level becomes corrupted, deleted, or otherwise inaccessible, the entire database may become unusable and unrecoverable. For these reasons, the KFS system seeks to create a series of size-capped databases where data is stored in a given \"shard\" based on a deterministic metric to ensure a sufficiently random and even spread to bound the cost of compaction, to reduce the impact of corruption, and to completely eliminate the need to maintain an index or state machine to efficiently lookup stored data. S-Buckets and Routing KFS requires that there be a reference identifier, which can be any arbitrary R bit key. This can be randomly generated upon creation of the database or derived from some other application or protocol specific information. In the Storj network, nodes are addressed with a 160 bit node identifier derived from the public portion of an ECDSA key pair. This Reference ID is used to calculate the database shard or S-Bucket to which a given piece of data belongs. Collectively, these S-Buckets form the B-Table. In KFS, there are a total of B S-Buckets, numbered 0-B-1. To determine which bucket a piece of raw binary data belongs in, calculate the [distance] between the first byte of the hash of the data and the first byte of the reference ID. This is to say that if the distance between those bytes is 137, then the raw binary data should be stored in S-Bucket 137. An S-Bucket has a fixed size, S, in bytes. This means that a KFS database has a maximum size of B * S bytes. Once an S-Bucket is full, no more data can be placed in it. Once a KFS database is full, another should be created using a new Reference ID. Given the default constants, KFS databases are capped at a maximum of 8TiB each. Keying Data by Chunks To optimize the efficiency of reads and writes in KFS, data is stored in C sized chunks (or less), keyed by the full content's hash, followed by a space and a numerical index. This is performed to ensure that key/value pairs are small and that reading and writing data to and from a S-Bucket is done sequentially and can allow for efficient streaming of data both in and out of the S-bucket. Since LevelDB sorts items lexicographically, keys for data chunks should be strings and consist of: Hexidecimal(Hash) + ' ' + 00000N The number of preceding zeroes in the numerical index should be set such that a S-Bucket that contains only a single file split into C sized chunks can still be read sequentially from the database. Using the default constants would make the highest number index 262144, so the number of leading zeroes should be less than or equal to five. Ad-Hoc S-Bucket Initialization Given the low cost of creating and opening a LevelDB, it is not necessary to create all B S-Buckets at once. Instead, an S-Bucket can be created the first time data is to be stored inside of it. Additionally, S-Buckets can be opened and closed as needed, eliminating the potential overhead of opening a large number of file descriptors. Operations on a given S-Bucket should be added to a queue which when drained may trigger a close of the S-Bucket's underlying database. Kademlia's metric for determining distance is defined as the result of the XOR operation on a set of bits interpreted as an integer. As such, for two randomly generated sets of bits, the result is uniformly distributed. Therefore the XOR distance between pseudo-random first bytes of the reference ID and hash give any bucket an equal chance of being selected. Below is the frequency distribution plotted with ten million simulated calculations. As expected the distribution is uniform (the red dotted line indicates the theoretical value each bin should have): Even with a uniform distribution, as the node reaches capacity some buckets will fill sooner than others. Offers that would be sorted into these buckets should be declined and relayed to other nodes. Constants Name Description Default B Number of columns in the B-table 256 S Size (in bytes) of an S-Bucket 34359738368 (32GiB) C Size (in bytes) of a file chunk 131072 R Number of bits in the Reference ID 160 Considerations Specific to Storj Storj farmers receive contracts for data shards that are already close to their own Node ID. To improve S-Bucket distribution, it may be desirable to double hash the data or otherwise force a degree of randomness before selecting a S-Bucket for storage. The use of KFS in the Storj network creates an upper limit to how much data can be stored by a given Node ID (or identity). This encourages farmers to operate multiple nodes with different identities which lends itself to better network integration. The use of HD (hierachical deterministic) private keys could allow a single farmer identity to assume multiple Reference IDs, thus eliminating the limit. KFS does not track or store metadata about the contents of a S-Bucket, which in the context of the Storj network would include contracts and other special information related to a piece of data. Applications should handle this via their own means. Ã— Search results Close "},"tutorial-cli.html":{"id":"tutorial-cli.html","title":"Tutorial: Command Line Interface","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Command Line Interface KFS comes bundles with a handy command line interface for dealing with your databases. You can access this tool by installing the package globally: npm install -g kfs Once the installation completes, you can use the kfs command. To see usage information, run: Usage: kfs [options] [command] Commands: write &lt;file_key&gt; [file_path] write the file to the database (or read from stdin) read &lt;file_key&gt; [file_path] read the file from the database (or write to stdout) unlink &lt;file_key&gt; unlink (delete) the file from the database list [options] &lt;bucket_index_or_file_index&gt; list all of the file keys in the given bucket stat [options] [bucket_index_or_file_key] get the free and used space for the database compact trigger a compaction of all database buckets * print usage information to the console Options: -h, --help output usage information -V, --version output the version number -d, --db &lt;db_path&gt; path the kfs database to use (default: /home/bookchin/.kfs/default) Writing a File To KFS There are two ways to write a file to a KFS database: Supplying an optional path to an existing file Reading from STDIN To write a file that exists on the file system already, just supply it's path: kfs write somefilekey /path/to/my/file.bin To have the CLI read from STDIN, just pipe the output of another program to it: cat /path/to/my/file.bin | kfs write somefilekey If an error is encountered, the process will terminate and write the error message to STDERR. Reading a File From KFS There are two ways to read a file from a KFS database: Supplying a path to write the output Writing to STDOUT To read from a KFS and write it to a file, just supply a path: kfs read somefilekey /path/to/write/file.webm To have the CLI write to STDOUT, just pipe the output to another program: kfs read somefilekey | mplayer - If an error is encountered, the process will terminate and write the error message to STDERR. Unlinking a File From KFS To unlink (or mark for deletion), simply provide the file key: kfs unlink somefilekey If an error is encountered, the process will terminate and write the error message to STDERR. Getting Stats for a KFS You can see the amount of space available for a given file key: kfs stat somefilekey 246.s 34359738368 This writes the S-bucket index and the number of bytes available to STDOUT. You can also view this in a human readable form with the -h option: kfs stat somefilekey -h 246.s 32.0 GiB Ã— Search results Close "},"tutorial-kfs.html":{"id":"tutorial-kfs.html","title":"Tutorial: Programmatic Usage","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Programmatic Usage This tutorial covers everything you need to know about using KFS within your application. KFS is based on LevelDB, an embedded key-value store, but the interface for interacting with a KFS is focused on the storage and retrieval of files and arbitrary binary streams. Getting Started To create and open a new KFS database (or open an existing one), simply require the module and create a Btable object: const kfs = require('kfs'); const myDataStore = kfs('/path/to/database.kfs'); That's it, Your data store is ready to use! Check if a File Exists To check if a file exists at a given key, use the Btable#exists method: const some160bitKey = 'adc83b19e793491b1c6ea0fd8b46cd9f32e592fc'; myDataStore.exists(some160bitKey, (err, exists) =&gt; { console.log('The file ' + (exists ? 'DOES' : 'DOES NOT') + ' exist!'); }); Check if a File Can Be Stored To check the available space for a file at a given key, use the Btable#getSpaceAvailableForKey method: const fileSizeInBytes = 4096; myDataStore.stat(some160bitKey, (err, result) =&gt; { if (err) { // handle error } let enoughFreeSpace = result.sBucketStats.free &gt; fileSizeInBytes; console.log('There ' + (enoughFreeSpace ? 'IS': 'IS NOT') + ' enough space!'); }); Write a File to the Data Store To write a raw buffer to the data store, use the Btable#writeFile method: const myFileBuffer = Buffer.from([/* ... */]); myDataStore.writeFile(some160bitKey, myFileBuffer, (err) =&gt; { console.log('File ' + (err ? 'WAS NOT' : 'WAS') + ' written!'); }); Read a File from the Data Store To read a file into memory from the data store, use the Btable#readFile method: myDataStore.readFile(some160bitKey, (err, fileBuffer) =&gt; { console.log(err || fileBuffer); }); Remove a File from the Data Store To remove a file from the data store, use the Btable#unlink method: myDataStore.unlink(some160bitKey, (err) =&gt; { console.log('The file ' + (err ? 'WAS NOT' : 'WAS') + ' removed!'); }); Use the Streaming Interfaces When reading or writing larger files, you may not wish to buffer everything into memory. In these cases, use the Btable#createReadStream and Btable#createWriteStream methods: myDataStore.createReadStream(some160bitKey, (err, readableStream) =&gt; { if (err) { // handle error } readableStream.on('data', (chunk) =&gt; { console.log('Got chunk:', chunk); }); readableStream.on('end', () =&gt; { console.log('All chunks read!'); }); readableStream.on('error', (err) =&gt; { console.log('Failed to read file:', err.message); }); }); myDataStore.createWriteStream(some160bitKey, (err, writableStream) =&gt; { if (err) { // handle error } writableStream.on('finish', () =&gt; { console.log('All chunks written!'); }); writableStream.on('error', (err) =&gt; { console.log('Failed to write file:', err.message); }); writableStream.write(Buffer.from([/* ... */])); writableStream.write(Buffer.from([/* ... */])); writableStream.write(Buffer.from([/* ... */])); writableStream.end(); }); Ã— Search results Close "},"tutorial-performance-testing.html":{"id":"tutorial-performance-testing.html","title":"Tutorial: Performance Testing the Changes","body":" KFS Modules kfskfs/constantskfs/utils Classes BlockStreamBtableReadableFileStreamSbucketWritableFileStream Events BlockStream#event:dataBlockStream#event:endReadableFileStream#event:dataReadableFileStream#event:endReadableFileStream#event:errorReadableFileStream#event:readableSbucket#event:closeSbucket#event:idleSbucket#event:lockedSbucket#event:openSbucket#event:unlockedWritableFileStream#event:errorWritableFileStream#event:finish Tutorials Motivation and MechanicsCommand Line InterfaceProgrammatic UsagePerformance Testing the Changes Performance Testing the Changes One major hypothesis of this project is that KFS enhances performance over the use a of a standard LevelDB instance. This is due to the nature of how KFS bounds the cost of LevelDB's compaction mechanism by sharding a data set over a series of size-capped LevelDB instances. A set of performance tests were run on a standard LevelDB along with our version which leverages KFS. This is a short summary of our findings and their implications. Experiment Design A series of one hundred trials were run in sequential order. Each trial consisted of measuring the execution time for a complete read, write, and unlink (delete) operation on file sizes of 8, 16, 32, 64, 128, 256 and 512 MiB. Keeping in mind that files are split into discrete 128KiB key/value pairs, keyed by a hash of the content of the entire file, this means that the actual number of read/write/delete operations are equal to the size of the file divided by 128KiB. Of particular note is that each sequential test run adds approximately 1GiB to the full size of the database (since unlinks only tombstone entries). Our number of trials is consistent with our assertion that LevelDBs performance degrades significantly after the size of the database exceeds 100GiB. This experiment was conducted for both a vanilla (standard) LevelDB and a version using the KFS protocol. In addition we ran the experiment using a hard disk drive (HDD) and solid state drive (SSD). Results An overview plot displaying the execution time by file size and operation for each trial indicates some difference between KFS and a vanilla LevelDB. At a high level it appears vanilla LevelDB had a higher variance across many categories. It is our belief that this variance is due to compaction triggering in LevelDB as the size of the single instance grows quickly. Since data is spread in a uniform fashion across a series of LevelDBs in KFS, this compaction triggering happens less frequently and has a much smaller impact. Upon closer inspection the data shows that in every category the mean execution time is lower for KFS for all categories. As for variance, the story is a bit more complicated. On SSD vanilla LevelDB has much greater variance than KFS for writes and unlinks but more consistent for reads. On HDD both KFS and vanilla show greater variance, but again KFS performs more consistently on writes and unlinks. Mean execution time comparison for SSD and HDD. Standard deviation execution time comparison for SSD and HDD. We ran two sided significant tests on each combination of operation and file size with a p-value cut-off at .05. For reads at 8, 16, 32, 64, 128 and 256 MiB file sizes, along with unlinks at 64 MiB we are unable to reject the null hypothesis. Or in other words, we are unable to suggest KFS performs better than a vanilla LevelDb in those scenarios. For the rest, we did achieve a 95% confidence level. This suggests that our measurements are not the result of a statistical fluke and KFS introduces meaningful change for those operations and file sizes. Please note that any confidence level of 100% is an artifact of rounding. In this scenario a p-value of 0 is theoretically impossible. Conclusion While P-Values should not be followed blindly, the data does indicate that the KFS protocol gives statistically significant gains in speed and consistency. To reproduce the data generated for these tests: Clone this git repository Make sure you have Node.js and NPM installed Run npm install from the project root directory Run npm run benchmark [iterations] [path_to_write_results] You can set the path to the database to create using the KFS_PERF_DIR environment variable for testing on different types of drives. If no path to write results to is specified, they will be written to stdout. If you want to experiment with chunk size (discrete key/value pairs associated with a file), modify the C constant in lib/constants.js. Ã— Search results Close "}}
    </script>

    <script type="text/javascript">
        $(document).ready(function() {
            Searcher.init();
        });

        $(window).on("message", function(msg) {
            var msgData = msg.originalEvent.data;

            if (msgData.msgid != "docstrap.quicksearch.start") {
                return;
            }

            var results = Searcher.search(msgData.searchTerms);

            window.parent.postMessage({"results": results, "msgid": "docstrap.quicksearch.done"}, "*");
        });
    </script>
</body>
</html>
